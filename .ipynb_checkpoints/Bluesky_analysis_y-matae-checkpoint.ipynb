{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9330a14e-3e48-4245-bb7d-a0dddae91a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cugraph\n",
    "import cudf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datashader as ds\n",
    "import cuxfilter\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import collections as cl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "298d486d-17d6-48a4-a39e-6b5a3cc7e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# ÁèæÂú®„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÂèñÂæó\n",
    "current_dir = os.path.dirname(os.path.abspath(\"Bluesky_analysis_y-matae.ipynb\"))\n",
    "\n",
    "# Ë¶™„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´ÁßªÂãï„Åó„Å¶JSON„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ„ÇíÊßãÁØâ\n",
    "json_file_202412161109 = os.path.join(current_dir, \"..\", \"bsky_data\", \"20241216\", \"11\", \"202412161109.json\")\n",
    "json_file_202412161110 = os.path.join(current_dir, \"..\", \"bsky_data\", \"20241216\", \"11\", \"202412161110.json\")\n",
    "\n",
    "# # JSON„Éï„Ç°„Ç§„É´„ÇíË°å„Åî„Å®„Å´Ë™≠„ÅøËæº„ÇÄ\n",
    "# with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         line = line.strip()  # Á©∫ÁôΩ„ÇíÂâäÈô§\n",
    "#         if line:  # Á©∫Ë°å„Çí„Çπ„Ç≠„ÉÉ„Éó\n",
    "#             try:\n",
    "#                 data = json.loads(line)  # ÂêÑË°å„ÇíJSON„Å®„Åó„Å¶Ë™≠„ÅøËæº„Åø\n",
    "#                 print(data)  # „Éá„Éº„Çø„ÇíÁ¢∫Ë™ç\n",
    "#             except json.JSONDecodeError as e:\n",
    "#                 print(f\"JSONDecodeError: {e}\")\n",
    "\n",
    "\n",
    "# # ‰∏ä„Åã„Çâ10‰ª∂„ÇíÂèñÂæó„Åó„Å¶Ë°®Á§∫\n",
    "# top_10_data = data[:10] if isinstance(data, list) else list(data.items())[:10]\n",
    "\n",
    "# # ÁµêÊûú„ÇíË°®Á§∫\n",
    "# for i, item in enumerate(top_10_data, start=1):\n",
    "#     print(f\"„Éá„Éº„Çø {i}: {item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e88b3e6-e354-4f42-a95a-6331c9c62800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊäΩÂá∫„Åï„Çå„Åü„Éù„Çπ„Éà„ÅÆ‰ª∂Êï∞: 1964\n",
      "„Éù„Çπ„Éà 1: {'did': 'did:plc:6btlrooty5fyxjukqiw423ey', 'time_us': 1734314940001178, 'kind': 'commit', 'commit': {'rev': '3ldfaoynyqm2o', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoyld4226', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.688Z', 'embed': {'$type': 'app.bsky.embed.record', 'record': {'cid': 'bafyreiek5dgblmt72m6pbvqihafihz4t6lb5gnalsb3ocnwcofcmnesfeq', 'uri': 'at://did:plc:dna3c6dbiajizvm2aionxled/app.bsky.feed.post/3ldf6kdcwz22p'}}, 'langs': ['en'], 'text': 'Fred Rococo 4ever'}, 'cid': 'bafyreieunlywcy243zsojwnw7rwvkc3vazqtth3dtcyr6iycna3qqtdb6y'}}\n",
      "„Éù„Çπ„Éà 2: {'did': 'did:plc:vu55lyfo64pq33fmvximr2pa', 'time_us': 1734314940009121, 'kind': 'commit', 'commit': {'rev': '3ldfaoxyqqs2q', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoxbsy22h', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:58.328Z', 'embed': {'$type': 'app.bsky.embed.external', 'external': {'description': \"ALT: a man with a beard is wearing a helmet that says ' x ' on it\", 'thumb': {'$type': 'blob', 'ref': {'$link': 'bafkreie353fxre5bydhpekponwwq53kkhiedzxgrauaiusg6iwuakj3qtu'}, 'mimeType': 'image/jpeg', 'size': 179188}, 'title': \"a man with a beard is wearing a helmet that says ' x ' on it\", 'uri': 'https://media.tenor.com/Da43_UhqMmAAAAAC/lotr-seriously.gif?hh=375&ww=498'}}, 'facets': [{'features': [{'$type': 'app.bsky.richtext.facet#link', 'uri': 'https://twitch.tv/isasava'}], 'index': {'byteEnd': 122, 'byteStart': 105}}], 'langs': ['fr'], 'text': 'Ce soir on essaie un premier jeu de survie de toute ma vie avec The Lord Of the Rings: Return to Moria \\n\\ntwitch.tv/isasava'}, 'cid': 'bafyreigij3rxcuhqo5kg5bwzjzwi7v3t2a7a7zpui2q2kajigqqltw27c4'}}\n",
      "„Éù„Çπ„Éà 3: {'did': 'did:plc:ftcxz244btgpfftnb5p4eusm', 'time_us': 1734314940009802, 'kind': 'commit', 'commit': {'rev': '3ldfaoylrrx2i', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaowtjx22w', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:57.861Z', 'langs': ['pt'], 'reply': {'parent': {'cid': 'bafyreigmswsqakvycmdtzmmssqikktmfimisytfwkm2ak5n22c7atmf4la', 'uri': 'at://did:plc:eovzsv5s2fd2anf5ddmnb55y/app.bsky.feed.post/3ldfamf6vhc2l'}, 'root': {'cid': 'bafyreigmswsqakvycmdtzmmssqikktmfimisytfwkm2ak5n22c7atmf4la', 'uri': 'at://did:plc:eovzsv5s2fd2anf5ddmnb55y/app.bsky.feed.post/3ldfamf6vhc2l'}}, 'text': 'Maluco ‚Äús√≥ queria ser mordido por um tubar√£o‚Äù‚Ä¶ sem condi√ß√µes kkkkkkkkkkkk'}, 'cid': 'bafyreicife7cixidjybai4vr5xpojuuqyxmlvipm5nssktscrwfe4s3upu'}}\n",
      "„Éù„Çπ„Éà 4: {'did': 'did:plc:2fn3jchl56lo7rlyhusg4qe7', 'time_us': 1734314940013377, 'kind': 'commit', 'commit': {'rev': '3ldfaoynlxp2y', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoxbn4k2e', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:58.322Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreicyyjk4rysehqqi4u2ndq5xdisu32yytvgkn7ytis36sab4z3pqfa', 'uri': 'at://did:plc:lz4f6fgoqlge4zuigionygjo/app.bsky.feed.post/3ldedmuomn22x'}, 'root': {'cid': 'bafyreidd2xk3nckwkv7w6hb5qprwidi4bpleukymjekr66o2gjw6aob7e4', 'uri': 'at://did:plc:iyz2tsas3di2yhjkwdptyvq5/app.bsky.feed.post/3ldcxhetgl22c'}}, 'text': 'Clearly the solution is heavy, locked rubber gloves~'}, 'cid': 'bafyreigsypim3lkakwwv74jcprxzrggvwlqvp64cxh2acr5hch6mre45by'}}\n",
      "„Éù„Çπ„Éà 5: {'did': 'did:plc:p4ztsa2ynzhgn3askfl4hreb', 'time_us': 1734314940017994, 'kind': 'commit', 'commit': {'rev': '3ldfaoyqqhv2f', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfap35iyc25', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:09:02.380Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreibinsrzxj74bl6i36wkvxebuukq4c2wc7xn7kqdgrjwfzvvf4yhpe', 'uri': 'at://did:plc:lcunjwvgmuvwkatty7lj4y6h/app.bsky.feed.post/3ldfamg23l227'}, 'root': {'cid': 'bafyreicw4fun3dkxuivztizolhcdpcqng2zuxdqjb3prqxreowhugi6ox4', 'uri': 'at://did:plc:p4ztsa2ynzhgn3askfl4hreb/app.bsky.feed.post/3lctmvofnwc27'}}, 'text': 'It was waaayyyy too good for a movie adaptation, and it even had its own unique bits of story, totally underrated üôèüî•'}, 'cid': 'bafyreibpbxtukygxh2d62mpbnrtjf6tjmbgm5notn4fuecyf2hxdluh7ti'}}\n",
      "„Éù„Çπ„Éà 6: {'did': 'did:plc:ovi65gazrbbtoh7rdr2zb4tf', 'time_us': 1734314940113099, 'kind': 'commit', 'commit': {'rev': '3ldfaoyfwyt23', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaovo7mc2g', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:56.635Z', 'embed': {'$type': 'app.bsky.embed.external', 'external': {'description': 'ALT: a man is crying while holding a microphone in his hand', 'thumb': {'$type': 'blob', 'ref': {'$link': 'bafkreicpzrm5l6jw6epzdrqd4gox72m25d33f3vw6bdbsx7cx2cn5rtnse'}, 'mimeType': 'image/jpeg', 'size': 980712}, 'title': 'a man is crying while holding a microphone in his hand', 'uri': 'https://media.tenor.com/pFKXm8b0drQAAAAC/rodrigo-faro-gugu.gif?hh=498&ww=498'}}, 'langs': ['pt'], 'reply': {'parent': {'cid': 'bafyreihe2bitwzqh55xdyqhen5lhwfinaoomz5gc4e2dl7jdserrjpc3zy', 'uri': 'at://did:plc:psxacwtvhmuaatpg2xz7k6ki/app.bsky.feed.post/3ldf7synng222'}, 'root': {'cid': 'bafyreihe2bitwzqh55xdyqhen5lhwfinaoomz5gc4e2dl7jdserrjpc3zy', 'uri': 'at://did:plc:psxacwtvhmuaatpg2xz7k6ki/app.bsky.feed.post/3ldf7synng222'}}, 'text': ''}, 'cid': 'bafyreiepqtiypxjlchqmycnyshwglbuejm5ll63zrfsdtq54vqcflx3n6q'}}\n",
      "„Éù„Çπ„Éà 7: {'did': 'did:plc:vqws3zws2zskclaaneqo6yiy', 'time_us': 1734314940113659, 'kind': 'commit', 'commit': {'rev': '3ldfaoyt5pf2g', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfap6fbcc2v', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:09:05.781Z', 'facets': [], 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreignpm3uoemxrc43nwdpmkw7nnf5cljglkwkxq6klzvyfe6jb62m6m', 'uri': 'at://did:plc:3i4mlyqcu6xmcqpe6rkrr5t4/app.bsky.feed.post/3ldfamtib722z'}, 'root': {'cid': 'bafyreibwzook7ai7qogpcc4uuwafna432cgez4ynlqfwbrqsfumjnrcfva', 'uri': 'at://did:plc:vqws3zws2zskclaaneqo6yiy/app.bsky.feed.post/3ldettzoatc2n'}}, 'text': \"See, you're just calling into question your love of all gummy bears here.\", 'via': 'TOKIMEKI'}, 'cid': 'bafyreifhggtcm6cj2yvgovu5g4i67d426mqx3j7zolyoxc3kn7sxjhn3uy'}}\n",
      "„Éù„Çπ„Éà 8: {'did': 'did:plc:ze7m36cpb2csesjjo76gnf6p', 'time_us': 1734314940204856, 'kind': 'commit', 'commit': {'rev': '3ldfaoytxq32l', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoyhc722z', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.556Z', 'langs': ['en'], 'text': 'Anthony Edwards is always so spicy when we‚Äôre winning at away games. It‚Äôs almost like he prefers having a crowd against him, likes proving them wrong. A spite fueled king.'}, 'cid': 'bafyreiaorqrynpibr2mwbohikf4suzo5dn2awm7tnafao4veezrd4fbk6i'}}\n",
      "„Éù„Çπ„Éà 9: {'did': 'did:plc:hv2jq4yasd42wuslick3fk3i', 'time_us': 1734314940206873, 'kind': 'commit', 'commit': {'rev': '3ldfaoyqdjt2u', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoyjem22x', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.624Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreidgomtptktx7yt7tvfunafgbh6t5zjmjkctrlznyu5xhtokvyg3qy', 'uri': 'at://did:plc:qwbui4trkz2ofrhyvyfroezh/app.bsky.feed.post/3ldf7wjfse22z'}, 'root': {'cid': 'bafyreibdlrrejlrwrp5s4v5p744j34rguy5yi5r7heigzuvyfrcysla6y4', 'uri': 'at://did:plc:xgiwtxbtt6xc7low5vdz7dq4/app.bsky.feed.post/3ldd35wmaqk2k'}}, 'text': 'sounds about right'}, 'cid': 'bafyreifoagaqehr2xxa2ilgty4ffjmtxbnizu5dh7ympng6z6lcna25tnq'}}\n",
      "„Éù„Çπ„Éà 10: {'did': 'did:plc:qvijapzahmcaz56gyawwihau', 'time_us': 1734314940210879, 'kind': 'commit', 'commit': {'rev': '3ldfaoyvf7h2z', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoycktc2g', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.402Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreihe7kuxio4qw2mycjupj5glozl45jyyxzzlnouxxy563msashj4ke', 'uri': 'at://did:plc:iitgew72r7qac3kzxhta4vr6/app.bsky.feed.post/3ldf7ve5aak22'}, 'root': {'cid': 'bafyreie56oa7gb6yzsls4krhntmyx36klivrc3rhgq2zrzhuvre42bfyiq', 'uri': 'at://did:plc:iitgew72r7qac3kzxhta4vr6/app.bsky.feed.post/3ldf6wqrm3c2p'}}, 'text': 'they lie i t was me'}, 'cid': 'bafyreigtox7qaclna3pjdken5j472swqxvjl2hctusfovd663zyhjbygpm'}}\n",
      "„Éù„Çπ„Éà 11: {'did': 'did:plc:guvokyie5rpu2ht7puye5647', 'time_us': 1734314940211197, 'kind': 'commit', 'commit': {'rev': '3ldfaoyx4fo2u', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoydf722h', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.429Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreih36b56czhxekjtusccmshbismemz7jfpwnrfqijzevahkgtn2tqu', 'uri': 'at://did:plc:bpodvps2da2ekoqp3pbmzhp5/app.bsky.feed.post/3ldfa7qtivs2x'}, 'root': {'cid': 'bafyreih36b56czhxekjtusccmshbismemz7jfpwnrfqijzevahkgtn2tqu', 'uri': 'at://did:plc:bpodvps2da2ekoqp3pbmzhp5/app.bsky.feed.post/3ldfa7qtivs2x'}}, 'text': 'omg i always forget i have neko atsume on my phone...'}, 'cid': 'bafyreidzwvzxpri3cs4wvbz3du7ug4w3z2hl3geswdtlx6tmpkbfsfto4a'}}\n",
      "„Éù„Çπ„Éà 12: {'did': 'did:plc:e4o626w2iff57fpven2onvv3', 'time_us': 1734314940222952, 'kind': 'commit', 'commit': {'rev': '3ldfaoywnge2x', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoyir3227', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.604Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreig3gvw7qw66c4lcqo7r4ap4zad2i2jm73tzyrbwliwjlk3fd5kk6a', 'uri': 'at://did:plc:5wgi4bw62voso56eovekf7ss/app.bsky.feed.post/3ldevewvhts2o'}, 'root': {'cid': 'bafyreig3gvw7qw66c4lcqo7r4ap4zad2i2jm73tzyrbwliwjlk3fd5kk6a', 'uri': 'at://did:plc:5wgi4bw62voso56eovekf7ss/app.bsky.feed.post/3ldevewvhts2o'}}, 'text': 'Yay! You excited for 5e?'}, 'cid': 'bafyreicbl5lkl3cfywsnzdkobjk7lhltep2bkq55yf4iydauzzp6lzw3uy'}}\n",
      "„Éù„Çπ„Éà 13: {'did': 'did:plc:ujd6lkum5xls76s2wwyglrcz', 'time_us': 1734314940223520, 'kind': 'commit', 'commit': {'rev': '3ldfaoyr5va2o', 'operation': 'delete', 'collection': 'app.bsky.feed.post', 'rkey': '3lcdpcy55dc24'}}\n",
      "„Éù„Çπ„Éà 14: {'did': 'did:plc:mowsqebzdtkyzp2m34l3643y', 'time_us': 1734314940302054, 'kind': 'commit', 'commit': {'rev': '3ldfaoyxbo52y', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoypumc2d', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.838Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreigextsajydz6gbmexhyge6jm5mz4tklzqlcu5mplcqtgz3bgku4au', 'uri': 'at://did:plc:xbsuw4ceyirmgtbscyptwgbe/app.bsky.feed.post/3ldf5uv35qh2i'}, 'root': {'cid': 'bafyreigextsajydz6gbmexhyge6jm5mz4tklzqlcu5mplcqtgz3bgku4au', 'uri': 'at://did:plc:xbsuw4ceyirmgtbscyptwgbe/app.bsky.feed.post/3ldf5uv35qh2i'}}, 'text': 'Cute!!'}, 'cid': 'bafyreiaot3ix6vosouwpcxhavei6vpipe7k6ovlgtcjcrn7zvkxuggzawm'}}\n",
      "„Éù„Çπ„Éà 15: {'did': 'did:plc:wnb6ex5ibn6jidfnqqyf7sun', 'time_us': 1734314940303258, 'kind': 'commit', 'commit': {'rev': '3ldfaoyyd4b2z', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoynjg22e', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.761Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreiaho3sm66kcqi4qfn2tmbx4nnrzogtrwqrcw7obh3mwydrdw45kqq', 'uri': 'at://did:plc:ykdaccwtlclkidjr5xcxb52n/app.bsky.feed.post/3ldeyondpuk26'}, 'root': {'cid': 'bafyreiaho3sm66kcqi4qfn2tmbx4nnrzogtrwqrcw7obh3mwydrdw45kqq', 'uri': 'at://did:plc:ykdaccwtlclkidjr5xcxb52n/app.bsky.feed.post/3ldeyondpuk26'}}, 'text': '‚úãüèæ‚úãüèæ'}, 'cid': 'bafyreifgylmupbnjkn43iayikhobkkkh3zzw5ns6bg2nz4qh3ebn3zi3v4'}}\n",
      "„Éù„Çπ„Éà 16: {'did': 'did:plc:w3t6ik5btu66smqkkqyxzsjh', 'time_us': 1734314940303580, 'kind': 'commit', 'commit': {'rev': '3ldfaoywbwc25', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoywuak2j', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:09:00.066Z', 'facets': [], 'langs': ['ja'], 'text': 'ÁóÖÈô¢‰∫àÁ¥Ñ„Åó„Åü„ÅÆ„ÅßÊ§úÊüª„Åó„Å¶„Åè„Çã', 'via': 'TOKIMEKI'}, 'cid': 'bafyreidckftsgkb6xhjq7uhvswfmdt7kxg7jaj2s2izqmfrtvjx7m6laa4'}}\n",
      "„Éù„Çπ„Éà 17: {'did': 'did:plc:32hcxdlcsszw2dku32sgscwi', 'time_us': 1734314940306530, 'kind': 'commit', 'commit': {'rev': '3ldfaoyw6xw27', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoye6lk2m', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.454Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreiazqenwg75gjuttrzgf47pndyjqj6hb3sxgx3fk7gvgewesd6knsu', 'uri': 'at://did:plc:s4dlrufms3pjpfzhuyvh6tlw/app.bsky.feed.post/3ld3umy6h2m2w'}, 'root': {'cid': 'bafyreiazqenwg75gjuttrzgf47pndyjqj6hb3sxgx3fk7gvgewesd6knsu', 'uri': 'at://did:plc:s4dlrufms3pjpfzhuyvh6tlw/app.bsky.feed.post/3ld3umy6h2m2w'}}, 'text': 'It‚Äôs really very sad.'}, 'cid': 'bafyreifyepucrhm2ngygnamdzuc3xixjukq2ccu76rlsljzb7p6grjpibe'}}\n",
      "„Éù„Çπ„Éà 18: {'did': 'did:plc:pocrqfk4pkkw2jebcd22iuxu', 'time_us': 1734314940314290, 'kind': 'commit', 'commit': {'rev': '3ldfaoyza6x2u', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaowc5ck2d', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:57.290Z', 'embed': {'$type': 'app.bsky.embed.external', 'external': {'description': 'Alt: a cartoon of two people hugging with the words sending virtual hug above them', 'thumb': {'$type': 'blob', 'ref': {'$link': 'bafkreigl7ghqrzz7qd3szygzttkgaqrfzjiknwtuvyywl2n3knn43m56k4'}, 'mimeType': 'image/jpeg', 'size': 110615}, 'title': 'a cartoon of two people hugging with the words sending virtual hug above them', 'uri': 'https://media.tenor.com/kbUyyUGpsagAAAAC/sending-hugs-sending-virtual-hug.gif?hh=498&ww=498'}}, 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreifazadr67txufhp5yqzrikfiawso37fa2yr4cyzjq6i3bfz26vjfq', 'uri': 'at://did:plc:dllhpteb7mollbgavtxtcd2h/app.bsky.feed.post/3ldfanaasgs2l'}, 'root': {'cid': 'bafyreidbe4tfd5mdrzf2vg5z5xord3togauh5367shsvnm36wt7e5xxkxu', 'uri': 'at://did:plc:pocrqfk4pkkw2jebcd22iuxu/app.bsky.feed.post/3ldfaklufe22d'}}, 'text': ''}, 'cid': 'bafyreiareq6xjoegrfhidzw6mtolxfdwi2klfl4nqp2sg42bdleopyvs5m'}}\n",
      "„Éù„Çπ„Éà 19: {'did': 'did:plc:ttcpkkhxv4a3zi2nk2w5lzab', 'time_us': 1734314940316731, 'kind': 'commit', 'commit': {'rev': '3ldfaoywtre2y', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfaoymhac2r', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:08:59.726Z', 'langs': ['en'], 'text': 'Tal vez s√≠, tal vez por costumbre sigo sufriendo por alguien que no puede o quiere estar conmigo.'}, 'cid': 'bafyreidgiun7ijc2w5qyyonqndxle3qjqlbest6cnainnzybnw7xlpj3oq'}}\n",
      "„Éù„Çπ„Éà 20: {'did': 'did:plc:dqnojxuekpcxkgaj2mdzhmog', 'time_us': 1734314940318288, 'kind': 'commit', 'commit': {'rev': '3ldfaoyztxq2y', 'operation': 'create', 'collection': 'app.bsky.feed.post', 'rkey': '3ldfap2hczk2y', 'record': {'$type': 'app.bsky.feed.post', 'createdAt': '2024-12-16T02:09:01.654Z', 'langs': ['en'], 'reply': {'parent': {'cid': 'bafyreibjzkz74fwhuzg7bdpgmu7gigddsxq3d4scgpb5ixg5jgkxlwf65u', 'uri': 'at://did:plc:dqnojxuekpcxkgaj2mdzhmog/app.bsky.feed.post/3ldfa46eqvk2y'}, 'root': {'cid': 'bafyreibh5kxnpo4wf32hipw5ltcpi2ta2zsof5e4kidjkxyt4v7ba42czq', 'uri': 'at://did:plc:nstawdlrl6hjcfttwnk4pohy/app.bsky.feed.post/3ldf7kvw25k2x'}}, 'text': \"That's just a YouTuber's opinions, link me to a real news article and I'll read it, but some guy on a podcast isn't evidence.\"}, 'cid': 'bafyreiedqk43n23vt2ifcmexhyykzgsiflxrrvon3sfefznb7ebk6bkjam'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„Åø„ÄÅ\"post\" „Å´Ë©≤ÂΩì„Åô„Çã„Éá„Éº„Çø„ÇíÊäΩÂá∫\n",
    "posts = []\n",
    "\n",
    "with open(json_file_202412161109, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()  # Á©∫ÁôΩ„ÇíÂâäÈô§\n",
    "        if line:  # Á©∫Ë°å„Çí„Çπ„Ç≠„ÉÉ„Éó\n",
    "            try:\n",
    "                data = json.loads(line)  # ÂêÑË°å„ÇíJSON„Å®„Åó„Å¶Ë™≠„ÅøËæº„Åø\n",
    "                # \"post\" „Å´Ë©≤ÂΩì„Åô„Çã„Éá„Éº„Çø„ÇíÊäΩÂá∫\n",
    "                if (\n",
    "                    \"commit\" in data\n",
    "                    and data[\"commit\"].get(\"collection\") == \"app.bsky.feed.post\"\n",
    "                ):\n",
    "                    posts.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSONDecodeError: {e}\")\n",
    "\n",
    "# ÊäΩÂá∫„Åó„Åü„Éá„Éº„Çø„ÇíË°®Á§∫\n",
    "print(f\"ÊäΩÂá∫„Åï„Çå„Åü„Éù„Çπ„Éà„ÅÆ‰ª∂Êï∞: {len(posts)}\")\n",
    "for i, post in enumerate(posts[:20], start=1):  # ‰∏ä‰Ωç10‰ª∂„ÇíË°®Á§∫\n",
    "    print(f\"„Éù„Çπ„Éà {i}: {post}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc6312e2-ae4c-4c26-b612-bad4406e5c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊäΩÂá∫„Åï„Çå„Åü„Éù„Çπ„Éà„ÅÆ‰ª∂Êï∞: 1774\n",
      "„Éù„Çπ„Éà 1: Fred Rococo 4ever\n",
      "„Éù„Çπ„Éà 2: Ce soir on essaie un premier jeu de survie de toute ma vie avec The Lord Of the Rings: Return to Moria \n",
      "\n",
      "twitch.tv/isasava\n",
      "„Éù„Çπ„Éà 3: Maluco ‚Äús√≥ queria ser mordido por um tubar√£o‚Äù‚Ä¶ sem condi√ß√µes kkkkkkkkkkkk\n",
      "„Éù„Çπ„Éà 4: Clearly the solution is heavy, locked rubber gloves~\n",
      "„Éù„Çπ„Éà 5: It was waaayyyy too good for a movie adaptation, and it even had its own unique bits of story, totally underrated üôèüî•\n",
      "„Éù„Çπ„Éà 6: See, you're just calling into question your love of all gummy bears here.\n",
      "„Éù„Çπ„Éà 7: Anthony Edwards is always so spicy when we‚Äôre winning at away games. It‚Äôs almost like he prefers having a crowd against him, likes proving them wrong. A spite fueled king.\n",
      "„Éù„Çπ„Éà 8: sounds about right\n",
      "„Éù„Çπ„Éà 9: they lie i t was me\n",
      "„Éù„Çπ„Éà 10: omg i always forget i have neko atsume on my phone...\n"
     ]
    }
   ],
   "source": [
    "# \"post\" „Å´Ë©≤ÂΩì„Åô„Çã„Éá„Éº„Çø„ÅÆ \"text\" „ÇíÊäΩÂá∫\n",
    "bluesky_texts = []\n",
    "with open(json_file_202412161109, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()  # Á©∫ÁôΩ„ÇíÂâäÈô§\n",
    "        if line:  # Á©∫Ë°å„Çí„Çπ„Ç≠„ÉÉ„Éó\n",
    "            try:\n",
    "                data = json.loads(line)  # ÂêÑË°å„ÇíJSON„Å®„Åó„Å¶Ë™≠„ÅøËæº„Åø\n",
    "                # \"post\" „Å´Ë©≤ÂΩì„Åô„Çã„Éá„Éº„Çø„ÇíÊäΩÂá∫\n",
    "                if (\n",
    "                    \"commit\" in data\n",
    "                    and data[\"commit\"].get(\"collection\") == \"app.bsky.feed.post\"\n",
    "                    and \"record\" in data[\"commit\"]\n",
    "                ):\n",
    "                    text = data[\"commit\"][\"record\"].get(\"text\")\n",
    "                    if text:\n",
    "                        bluesky_texts.append(text)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSONDecodeError: {e}\")\n",
    "\n",
    "print(f\"ÊäΩÂá∫„Åï„Çå„Åü„Éù„Çπ„Éà„ÅÆ‰ª∂Êï∞: {len(bluesky_texts)}\")\n",
    "\n",
    "# ‰∏ä‰Ωç10‰ª∂„ÇíÁ¢∫Ë™ç\n",
    "for i, t in enumerate(bluesky_texts[:10], start=1):\n",
    "    print(f\"„Éù„Çπ„Éà {i}: {t}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66d70659-2c88-4c3f-8588-19dddaa80b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# TF-IDF„ÅÆË®àÁÆóÈÉ®ÂàÜ\n",
    "# -----------------------\n",
    "# 1. TfidfVectorizer„Åßtexts„Çí„Éô„ÇØ„Éà„É´Âåñ\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(bluesky_texts)\n",
    "\n",
    "# 2. ÊäΩÂá∫„Åó„ÅüÂçòË™û„É™„Çπ„Éà\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 3. ÂÖ®‰Ωì„ÅÆTF-IDF„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó\n",
    "total_tfidf_scores = np.sum(tfidf_matrix.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8b1a25a-bf4c-4d72-aff5-0270be484cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ÂÖ®‰Ωì„ÅÆTF-IDF„Çπ„Ç≥„Ç¢(‰∏ä‰Ωç20) ===\n",
      "the: 60.731\n",
      "to: 46.015\n",
      "you: 38.902\n",
      "and: 37.668\n",
      "it: 34.560\n",
      "of: 32.638\n",
      "is: 31.870\n",
      "that: 31.439\n",
      "in: 28.774\n",
      "this: 26.727\n",
      "for: 26.030\n",
      "my: 25.844\n",
      "on: 20.944\n",
      "com: 20.545\n",
      "me: 19.091\n",
      "be: 18.921\n",
      "so: 17.279\n",
      "with: 17.148\n",
      "just: 16.869\n",
      "are: 16.856\n",
      "\n",
      "=== ÂÖ®‰Ωì„ÅÆTF-IDF„Çπ„Ç≥„Ç¢: DataFrame ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ÂçòË™û</th>\n",
       "      <th>ÂÖ®‰Ωì„Çπ„Ç≥„Ç¢</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>60.731110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>46.015477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you</td>\n",
       "      <td>38.901743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>37.667575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it</td>\n",
       "      <td>34.559804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>32.637771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is</td>\n",
       "      <td>31.870397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>that</td>\n",
       "      <td>31.439126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>28.773915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>26.727471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>for</td>\n",
       "      <td>26.030244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>my</td>\n",
       "      <td>25.843948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>on</td>\n",
       "      <td>20.944458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>com</td>\n",
       "      <td>20.545441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>me</td>\n",
       "      <td>19.090547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>be</td>\n",
       "      <td>18.921385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>so</td>\n",
       "      <td>17.278573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>with</td>\n",
       "      <td>17.147890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>just</td>\n",
       "      <td>16.868839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>are</td>\n",
       "      <td>16.855548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ÂçòË™û      ÂÖ®‰Ωì„Çπ„Ç≥„Ç¢\n",
       "0    the  60.731110\n",
       "1     to  46.015477\n",
       "2    you  38.901743\n",
       "3    and  37.667575\n",
       "4     it  34.559804\n",
       "5     of  32.637771\n",
       "6     is  31.870397\n",
       "7   that  31.439126\n",
       "8     in  28.773915\n",
       "9   this  26.727471\n",
       "10   for  26.030244\n",
       "11    my  25.843948\n",
       "12    on  20.944458\n",
       "13   com  20.545441\n",
       "14    me  19.090547\n",
       "15    be  18.921385\n",
       "16    so  17.278573\n",
       "17  with  17.147890\n",
       "18  just  16.868839\n",
       "19   are  16.855548"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 4. ÂçòË™û„Åî„Å®„ÅÆTF-IDF„Çπ„Ç≥„Ç¢„ÇíËæûÊõ∏ÂΩ¢Âºè„Å´„Åæ„Å®„ÇÅ„Çã\n",
    "overall_scores = {\n",
    "    feature_names[i]: total_tfidf_scores[i] \n",
    "    for i in range(len(feature_names))\n",
    "}\n",
    "\n",
    "# 5. ÂÖ®‰Ωì„ÅÆ„Çπ„Ç≥„Ç¢„ÇíÈôçÈ†Ü„Åß„ÇΩ„Éº„Éà\n",
    "sorted_scores = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 6. ‰∏ä‰Ωç20ÂçòË™û„ÅÆTF-IDF„Çπ„Ç≥„Ç¢„ÇíË°®Á§∫ÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶Êï∞„ÇíË™øÊï¥Ôºâ\n",
    "print(\"\\n=== ÂÖ®‰Ωì„ÅÆTF-IDF„Çπ„Ç≥„Ç¢(‰∏ä‰Ωç20) ===\")\n",
    "for word, score in sorted_scores[:20]:\n",
    "    print(f\"{word}: {score:.3f}\")\n",
    "\n",
    "# -----------------------\n",
    "# „Éá„Éº„Çø„Éï„É¨„Éº„É†Âåñ„Åó„Å¶Ë°®Á§∫\n",
    "# -----------------------\n",
    "overall_tfidf_df = pd.DataFrame({\n",
    "    \"ÂçòË™û\": list(overall_scores.keys()),\n",
    "    \"ÂÖ®‰Ωì„Çπ„Ç≥„Ç¢\": list(overall_scores.values())\n",
    "})\n",
    "\n",
    "# „Çπ„Ç≥„Ç¢„Åß„ÇΩ„Éº„Éà\n",
    "overall_tfidf_df.sort_values(\"ÂÖ®‰Ωì„Çπ„Ç≥„Ç¢\", ascending=False, inplace=True)\n",
    "overall_tfidf_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\n=== ÂÖ®‰Ωì„ÅÆTF-IDF„Çπ„Ç≥„Ç¢: DataFrame ===\")\n",
    "display(overall_tfidf_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d07b4d6-2ab7-443f-8894-efaf2b813c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3294761-581d-4b34-beb6-330c847a0fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/y-matae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/y-matae/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/y-matae/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')       # „Éà„Éº„ÇØ„É≥ÂàÜÂâ≤Áî®\n",
    "nltk.download('stopwords')   # Ëã±Ë™û„ÅÆ„Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„Éâ\n",
    "nltk.download('wordnet')     # „É¨„É≥„ÉûÂåñ(WordNetLemmatizer)Áî®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01a04bfa-a4c3-4709-967f-c955f49eec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ëã±Ë™û„ÅÆ„Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„Éâ„Çí„Çª„ÉÉ„Éà„ÅßÂèñÂæó\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# ËøΩÂä†„ÅßÈô§Â§ñ„Åó„Åü„ÅÑÂçòË™ûÔºà‰ªªÊÑèÔºâ\n",
    "# ‰æã: 'be' „ÅØÂæå„ÅßËá™ÂãïÁöÑ„Å´Èô§Â§ñ„Åô„Çã„Åå„ÄÅ'am', 'are' „ÇíÁõ¥Êé•Èô§Â§ñ„Åó„Åü„ÅÑÂ†¥Âêà„Å™„Å©\n",
    "# custom_words = {'be', 'am', 'are', 'was', 'were'}\n",
    "\n",
    "# „É¨„É≥„Éû„Çø„Ç§„Ç∂„ÇíÁî®ÊÑè\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text_nltk(text: str) -> str:\n",
    "    \"\"\"\n",
    "    NLTK„Çí‰Ωø„Å£„Å¶„ÉÜ„Ç≠„Çπ„Éà„Çí‰ª•‰∏ã„ÅÆÊâãÈ†Ü„ÅßÂâçÂá¶ÁêÜ:\n",
    "      1. Â∞èÊñáÂ≠óÂåñ\n",
    "      2. „Éà„Éº„ÇØ„É≥ÂàÜÂâ≤ (word_tokenize)\n",
    "      3. „É¨„É≥„ÉûÂåñ (WordNetLemmatizer)\n",
    "      4. „Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„Éâ„ÉªbeÂãïË©û„ÉªÂè•Ë™≠ÁÇπ„ÇÑÊï∞Â≠ó„Å™„Å©„ÇíÈô§Âéª\n",
    "      5. „Çπ„Éö„Éº„ÇπÂå∫Âàá„Çä„ÅßÁµêÂêà\n",
    "    \"\"\"\n",
    "    # 1. Â∞èÊñáÂ≠óÂåñ & „Éà„Éº„ÇØ„É≥ÂàÜÂâ≤\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        # 2. „É¨„É≥„ÉûÂåñ (ÂãïË©ûÂÑ™ÂÖà„ÅßÊ≠£Ë¶èÂåñ‚ÜíÂêçË©û„ÇÇË©¶„Åô‰æã)\n",
    "        #   ex) 'am', 'are', 'was', 'were' -> lemma: \"be\"\n",
    "        #   ex) 'running' -> lemma: \"run\"\n",
    "        # WordNetLemmatizer„ÅØPOS„ÇíÊòéÁ§∫„Åô„Çã„Å®Á≤æÂ∫¶„Åå‰∏ä„Åå„Çã\n",
    "        lemma_v = lemmatizer.lemmatize(token, pos='v')  # ÂãïË©û„Å®„Åó„Å¶\n",
    "        lemma_n = lemmatizer.lemmatize(lemma_v, pos='n')  # ÂêçË©û„Å®„Åó„Å¶\n",
    "        lemma = lemma_n  # ÊúÄÁµÇÁöÑ„Å´ lemma „Å®„Åó„Å¶Êé°Áî®\n",
    "        \n",
    "        # 3. „Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„Éâ„ÇíÈô§Â§ñ (\"the\", \"a\", \"is\"„Å™„Å©)\n",
    "        if lemma in nltk_stopwords:\n",
    "            continue\n",
    "        \n",
    "        # 4. \"be\" (am, are, was, were... „Å™„Å©) „Å´„Å™„Å£„Åü„Éà„Éº„ÇØ„É≥„ÇíÈô§Â§ñ\n",
    "        if lemma == \"be\":\n",
    "            continue\n",
    "        \n",
    "        # 5. „Ç¢„É´„Éï„Ç°„Éô„ÉÉ„Éà„ÅÆ„ÅøÊÆã„Åô(Âè•Ë™≠ÁÇπ„ÇÑÊï∞Â≠ó„ÇíÈô§Â§ñ„Åó„Åü„ÅÑÂ†¥Âêà)\n",
    "        #   ‰æã: \"hello,\" -> \"hello\" „Å®„Åó„Åü„ÅÑÂ†¥Âêà„ÅØ„Åï„Çâ„Å´Êï¥ÂΩ¢„Åô„Çã„Åã„ÄÅ\n",
    "        #   isalpha() „ÅßËã±Â≠ó„ÅÆ„ÅøÂà§ÂÆö„Åô„Çã\n",
    "        if not lemma.isalpha():\n",
    "            continue\n",
    "        \n",
    "        processed_tokens.append(lemma)\n",
    "    \n",
    "    # 6. „Çπ„Éö„Éº„ÇπÂå∫Âàá„Çä„ÅÆÊñáÂ≠óÂàó„Å´Êàª„Åô\n",
    "    return \" \".join(processed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d38174fb-ad88-487c-922b-7005084aec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/y-matae/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# === ÂâçÂá¶ÁêÜ„ÇíÈÅ©Áî® ===\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "processed_texts = [preprocess_text_nltk(t) for t in bluesky_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "250da302-5170-46e8-98db-817bf9c692f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info] ÊñáÊõ∏Êï∞: 1774\n",
      "[Info] ÁâπÂæ¥Êï∞(ÂçòË™ûÊï∞): 5711\n"
     ]
    }
   ],
   "source": [
    "# === TF-IDF „Éô„ÇØ„Éà„É´Âåñ ===\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"\\n[Info] ÊñáÊõ∏Êï∞: {tfidf_matrix.shape[0]}\")\n",
    "print(f\"[Info] ÁâπÂæ¥Êï∞(ÂçòË™ûÊï∞): {tfidf_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "019628ce-778a-4361-9746-0a22dd97513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÖ®‰Ωì„Çπ„Ç≥„Ç¢„ÇíÂêàË®à„Åó„Å¶‰∏ä‰ΩçÂçòË™û„ÇíË°®Á§∫\n",
    "total_tfidf_scores = np.sum(tfidf_matrix.toarray(), axis=0)\n",
    "score_dict = dict(zip(feature_names, total_tfidf_scores))\n",
    "sorted_scores = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8235890c-b9ae-461d-9cac-cad96dd48291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF„Çπ„Ç≥„Ç¢‰∏ä‰Ωç20ÂçòË™û ===\n",
      "get: 21.905\n",
      "like: 19.934\n",
      "love: 19.001\n",
      "one: 18.222\n",
      "time: 14.785\n",
      "make: 14.389\n",
      "go: 13.313\n",
      "say: 12.608\n",
      "de: 12.572\n",
      "thank: 12.438\n",
      "see: 12.348\n",
      "look: 12.146\n",
      "think: 11.883\n",
      "know: 11.654\n",
      "http: 11.498\n",
      "good: 11.331\n",
      "want: 11.294\n",
      "need: 11.111\n",
      "would: 10.923\n",
      "really: 10.583\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TF-IDF„Çπ„Ç≥„Ç¢‰∏ä‰Ωç20ÂçòË™û ===\")\n",
    "for word, score in sorted_scores[:20]:\n",
    "    print(f\"{word}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3c3a101-38df-4aea-aa65-5d700decf439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DataFrame „Åß‰∏ä‰Ωç20ÂçòË™û„ÇíÁ¢∫Ë™ç ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get</td>\n",
       "      <td>21.905325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>19.934493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love</td>\n",
       "      <td>19.000765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one</td>\n",
       "      <td>18.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time</td>\n",
       "      <td>14.784653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>make</td>\n",
       "      <td>14.388975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>go</td>\n",
       "      <td>13.312586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>say</td>\n",
       "      <td>12.608133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>de</td>\n",
       "      <td>12.571971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thank</td>\n",
       "      <td>12.437794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>see</td>\n",
       "      <td>12.347806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>look</td>\n",
       "      <td>12.145572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>think</td>\n",
       "      <td>11.882891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>know</td>\n",
       "      <td>11.653703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http</td>\n",
       "      <td>11.498412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>good</td>\n",
       "      <td>11.331152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>want</td>\n",
       "      <td>11.294099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>need</td>\n",
       "      <td>11.110992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>would</td>\n",
       "      <td>10.922620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>really</td>\n",
       "      <td>10.583263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word      score\n",
       "0      get  21.905325\n",
       "1     like  19.934493\n",
       "2     love  19.000765\n",
       "3      one  18.222300\n",
       "4     time  14.784653\n",
       "5     make  14.388975\n",
       "6       go  13.312586\n",
       "7      say  12.608133\n",
       "8       de  12.571971\n",
       "9    thank  12.437794\n",
       "10     see  12.347806\n",
       "11    look  12.145572\n",
       "12   think  11.882891\n",
       "13    know  11.653703\n",
       "14    http  11.498412\n",
       "15    good  11.331152\n",
       "16    want  11.294099\n",
       "17    need  11.110992\n",
       "18   would  10.922620\n",
       "19  really  10.583263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DataFrameË°®Á§∫ (Pandas)\n",
    "df_tfidf = pd.DataFrame({\n",
    "    \"word\": list(score_dict.keys()),\n",
    "    \"score\": list(score_dict.values())\n",
    "}).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== DataFrame „Åß‰∏ä‰Ωç20ÂçòË™û„ÇíÁ¢∫Ë™ç ===\")\n",
    "display(df_tfidf.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9dcbc-342b-49bf-8b05-342e8eb4122e",
   "metadata": {},
   "source": [
    "##### Êñ∞Âπ¥„ÅÆÊå®Êã∂ÂàÜÊûê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ba6881d-7f62-4ef3-8376-ff5cceef9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ë¶™„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´ÁßªÂãï„Åó„Å¶JSON„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ„ÇíÊßãÁØâ\n",
    "json_file_202501010000 = os.path.join(current_dir, \"..\", \"bsky_data\", \"20250101\", \"00\", \"202501010000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef320751-be9f-48b6-8df0-d513998cdd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ë™≠„ÅøËæº„Çì„Å†ÊäïÁ®øÊï∞: 8948‰ª∂\n",
      "[1] Pretty angel. üåπ\n",
      "\n",
      "[2] Posting a favorite photo from each month in 2024. Jan 24: This is the annual tree burn after the holidays at #AlkiBeach. My fav local event.\n",
      "\n",
      "[3] „ÅÇ„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅÔºÅÔºÅÔºàÁ¥†ÊåØ„ÇäÔºâ\n",
      "\n",
      "[4] Baby on the right looks so sad üò≠\n",
      "\n",
      "[5] „ÅÇ„Åë„Åä„ÇÅ„Éº„Éº\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# „Éù„Çπ„ÉàÊú¨Êñá„ÇíÊ†ºÁ¥ç„Åô„Çã„É™„Çπ„Éà\n",
    "posts_202501010000 = []\n",
    "\n",
    "# JSON„Éï„Ç°„Ç§„É´„ÇíÈñã„ÅÑ„Å¶1Ë°å„Åö„Å§Ë™≠„ÅøËæº„ÇÄ\n",
    "with open(json_file_202501010000, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Á©∫Ë°å„Çí„Çπ„Ç≠„ÉÉ„Éó\n",
    "        try:\n",
    "            data = json.loads(line)  # JSON„Éá„Éº„Çø„Å´Â§âÊèõ\n",
    "            # Bluesky„Åß„ÅØ \"commit\" ‚Üí \"collection\" „Å´ \"app.bsky.feed.post\" „Åå„ÅÇ„Çå„Å∞„ÄåÊäïÁ®ø„Éá„Éº„Çø„Äç„Å®„Åø„Å™„Åô\n",
    "            if (\"commit\" in data \n",
    "                and data[\"commit\"].get(\"collection\") == \"app.bsky.feed.post\"\n",
    "                and \"record\" in data[\"commit\"]):\n",
    "                \n",
    "                # ÊäïÁ®øÊú¨ÊñáÔºàtext„Éï„Ç£„Éº„É´„ÉâÔºâ„ÇíÂèñÂæó\n",
    "                text_field = data[\"commit\"][\"record\"].get(\"text\")\n",
    "                if text_field:\n",
    "                    posts_202501010000.append(text_field)\n",
    "                    \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError: {e}\")\n",
    "\n",
    "# ÂèñÂæó„Åó„ÅüÊäïÁ®ø„ÅÆ‰ª∂Êï∞„ÇíÁ¢∫Ë™ç\n",
    "print(f\"Ë™≠„ÅøËæº„Çì„Å†ÊäïÁ®øÊï∞: {len(posts_202501010000)}‰ª∂\")\n",
    "\n",
    "# ‰∏ä‰ΩçÊï∞‰ª∂„Çí„Çµ„É≥„Éó„É´Ë°®Á§∫ÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶Ôºâ\n",
    "for i, post_text in enumerate(posts_202501010000[:5], start=1):\n",
    "    print(f\"[{i}] {post_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06dc1b33-4a3b-45eb-81ff-8e51ccf785b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langdetect in /home/y-matae/.local/lib/python3.12/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b752372-e37f-4da5-ae20-fe50230a8df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Êó•Êú¨Ë™û‰ª•Â§ñ„ÅÆ„ÅÇ„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Éù„Çπ„ÉàÊï∞: 348\n"
     ]
    }
   ],
   "source": [
    "#Êñ∞Âπ¥„ÅÆ„ÅÇ„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Éù„Çπ„Éà„ÅÆÊï∞\n",
    "import os\n",
    "import json\n",
    "from langdetect import detect, DetectorFactory, LangDetectException\n",
    "\n",
    "# langdetect „Åå„Çπ„É¨„ÉÉ„ÉâÈùû‰æùÂ≠ò„ÅÆÁµêÊûú„ÇíËøî„Åô„Çà„ÅÜ„Å´„Åô„Çã\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# 1. „Éñ„É´„Éº„Çπ„Ç´„Ç§ÊäïÁ®ø„ÅåÂÖ•„Å£„Åü„É™„Çπ„Éà (ÂâçÊÆµ„ÅßÂèñÂæóÊ∏à„Åø)\n",
    "#    ‰æã: posts_202501010000 = [... „Åô„Åß„Å´ \"text\" „ÇíÊ†ºÁ¥çÊ∏à„Åø ...]\n",
    "\n",
    "# 2. ‰ªñË®ÄË™û„ÅÆÊñ∞Âπ¥Êå®Êã∂„Éï„É¨„Éº„Ç∫„ÅÆ„É™„Çπ„ÉàÔºà„ÅÇ„Åè„Åæ„Åß‰æãÁ§∫Ôºâ\n",
    "newyear_phrases = [\n",
    "    \"happy new year\",  # Ëã±Ë™û\n",
    "    \"bonne ann√©e\",     # „Éï„É©„É≥„ÇπË™û\n",
    "    \"feliz a√±o nuevo\", # „Çπ„Éö„Ç§„É≥Ë™û\n",
    "    \"feliz ano novo\",  # „Éù„É´„Éà„Ç¨„É´Ë™û\n",
    "    \"ein frohes neues jahr\", # „Éâ„Ç§„ÉÑË™û\n",
    "    \"buon anno\",       # „Ç§„Çø„É™„Ç¢Ë™û\n",
    "    \"ÏÉàÌï¥ Î≥µ ÎßéÏù¥ Î∞õÏúºÏÑ∏Ïöî\",   # ÈüìÂõΩË™û\n",
    "    \"—Å –Ω–æ–≤—ã–º –≥–æ–¥–æ–º\",   # „É≠„Ç∑„Ç¢Ë™û (S Novim Godom)\n",
    "    # ... ÂøÖË¶Å„Å´Âøú„Åò„Å¶ËøΩÂä† ...\n",
    "]\n",
    "\n",
    "non_japanese_count = 0\n",
    "\n",
    "for post_text in posts_202501010000:\n",
    "    text_lower = post_text.lower()  # Â∞èÊñáÂ≠óÂåñ„Åó„Å¶Ê§úÁ¥¢„ÇíÂçòÁ¥îÂåñ\n",
    "    \n",
    "    # 3. Ë®ÄË™ûÂà§ÂÆö\n",
    "    try:\n",
    "        lang = detect(post_text)\n",
    "    except LangDetectException:\n",
    "        # Ëß£Êûê„ÅåÂõ∞Èõ£„Å™Â†¥Âêà„ÅØ„Çπ„Ç≠„ÉÉ„Éó\n",
    "        continue\n",
    "    \n",
    "    # 4. Êó•Êú¨Ë™û‰ª•Â§ñ„Åß„ÄÅ„Åã„Å§„Éï„É¨„Éº„Ç∫„ÅåÂê´„Åæ„Çå„Çã„Åã\n",
    "    if lang != \"ja\":\n",
    "        # „ÅÑ„Åö„Çå„Åã„ÅÆÊñ∞Âπ¥Êå®Êã∂„Éï„É¨„Éº„Ç∫„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çå„Å∞„Ç´„Ç¶„É≥„Éà\n",
    "        if any(phrase in text_lower for phrase in newyear_phrases):\n",
    "            non_japanese_count += 1\n",
    "\n",
    "print(f\"Êó•Êú¨Ë™û‰ª•Â§ñ„ÅÆ„ÅÇ„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Éù„Çπ„ÉàÊï∞: {non_japanese_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81939748-ae29-4a4b-8405-361f544757d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"tensorflow>=2.0\" tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19489ba6-d4f2-40e6-8f91-43d918df6506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶Ë°åÂàó ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post0</th>\n",
       "      <th>Post1</th>\n",
       "      <th>Post2</th>\n",
       "      <th>Post3</th>\n",
       "      <th>Post4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Post0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Post1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Post2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Post3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Post4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Post0  Post1  Post2  Post3  Post4\n",
       "Post0    1.0    0.0    0.0    0.0    0.0\n",
       "Post1    0.0    1.0    0.0    0.0    0.0\n",
       "Post2    0.0    0.0    1.0    0.0    0.0\n",
       "Post3    0.0    0.0    0.0    1.0    0.0\n",
       "Post4    0.0    0.0    0.0    0.0    1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ‰ºº„Å¶„ÅÑ„ÇãÊäïÁ®ø„Éö„Ç¢ (‰∏ä‰Ωç10) ===\n",
      "Post0 vs Post1 : similarity=0.000\n",
      "   - Êòé„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅ‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ\n",
      "   - „ÅÇ„Åë„Åä„ÇÅ„Åß„ÅôÔºÅÁöÜ„Åï„Çì‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„Éº\n",
      "\n",
      "Post0 vs Post2 : similarity=0.000\n",
      "   - Êòé„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅ‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ\n",
      "   - Happy new year to everyone! Wishing you all the best.\n",
      "\n",
      "Post0 vs Post3 : similarity=0.000\n",
      "   - Êòé„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅ‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ\n",
      "   - Bonne ann√©e! Meilleurs v≈ìux √† tous!\n",
      "\n",
      "Post0 vs Post4 : similarity=0.000\n",
      "   - Êòé„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅ‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ\n",
      "   - „ÅÇ„Åë„Åä„ÇÅÔºÅ‰ªäÂπ¥„ÅÆÁõÆÊ®ô„ÇíË™û„Çä„Åæ„Åó„Çá„ÅÜÔºÅ\n",
      "\n",
      "Post1 vs Post2 : similarity=0.000\n",
      "   - „ÅÇ„Åë„Åä„ÇÅ„Åß„ÅôÔºÅÁöÜ„Åï„Çì‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„Éº\n",
      "   - Happy new year to everyone! Wishing you all the best.\n",
      "\n",
      "Post1 vs Post3 : similarity=0.000\n",
      "   - „ÅÇ„Åë„Åä„ÇÅ„Åß„ÅôÔºÅÁöÜ„Åï„Çì‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„Éº\n",
      "   - Bonne ann√©e! Meilleurs v≈ìux √† tous!\n",
      "\n",
      "Post1 vs Post4 : similarity=0.000\n",
      "   - „ÅÇ„Åë„Åä„ÇÅ„Åß„ÅôÔºÅÁöÜ„Åï„Çì‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„Éº\n",
      "   - „ÅÇ„Åë„Åä„ÇÅÔºÅ‰ªäÂπ¥„ÅÆÁõÆÊ®ô„ÇíË™û„Çä„Åæ„Åó„Çá„ÅÜÔºÅ\n",
      "\n",
      "Post2 vs Post3 : similarity=0.000\n",
      "   - Happy new year to everyone! Wishing you all the best.\n",
      "   - Bonne ann√©e! Meilleurs v≈ìux √† tous!\n",
      "\n",
      "Post2 vs Post4 : similarity=0.000\n",
      "   - Happy new year to everyone! Wishing you all the best.\n",
      "   - „ÅÇ„Åë„Åä„ÇÅÔºÅ‰ªäÂπ¥„ÅÆÁõÆÊ®ô„ÇíË™û„Çä„Åæ„Åó„Çá„ÅÜÔºÅ\n",
      "\n",
      "Post3 vs Post4 : similarity=0.000\n",
      "   - Bonne ann√©e! Meilleurs v≈ìux √† tous!\n",
      "   - „ÅÇ„Åë„Åä„ÇÅÔºÅ‰ªäÂπ¥„ÅÆÁõÆÊ®ô„ÇíË™û„Çä„Åæ„Åó„Çá„ÅÜÔºÅ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Êó•Êú¨Ë™û„ÅÆ„Åø„ÅßË°å„ÅÜ‰æãÔºàMecab„ÅÆÂ∞éÂÖ•„ÅåÂøÖË¶ÅÔºâ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. „ÅÇ„Åë„Åä„ÇÅ„Éù„Çπ„Éà„ÅÆ„É™„Çπ„Éà„Çí‰ªÆÂÆö\n",
    "# Ôºà„Åô„Åß„Å´„Äå„ÅÇ„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Äç„Å£„ÅΩ„ÅÑÊäïÁ®ø„ÇíÊäΩÂá∫Ê∏à„Åø„Å†„Å®„Åô„ÇãÔºâ\n",
    "posts_akemashite_omedetou = [\n",
    "    \"Êòé„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅ‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ\",\n",
    "    \"„ÅÇ„Åë„Åä„ÇÅ„Åß„ÅôÔºÅÁöÜ„Åï„Çì‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„Éº\",\n",
    "    \"Happy new year to everyone! Wishing you all the best.\",\n",
    "    \"Bonne ann√©e! Meilleurs v≈ìux √† tous!\",\n",
    "    \"„ÅÇ„Åë„Åä„ÇÅÔºÅ‰ªäÂπ¥„ÅÆÁõÆÊ®ô„ÇíË™û„Çä„Åæ„Åó„Çá„ÅÜÔºÅ\",\n",
    "    # ... „Åï„Çâ„Å´Â§öÊï∞„ÅÆÊäïÁ®ø ...\n",
    "]\n",
    "\n",
    "# 2. TF-IDF „Éô„ÇØ„Éà„É´Âåñ\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(posts_akemashite_omedetou)\n",
    "# shape = (N‰ª∂„ÅÆÊäïÁ®ø, MÊ¨°ÂÖÉ)\n",
    "\n",
    "# 3. „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶„ÇíË®àÁÆó\n",
    "# Ë°åÂàóÂêåÂ£´„Åß„Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶„ÇíË®àÁÆó„Åó„ÄÅ(N x N) „ÅÆÈ°û‰ººÂ∫¶Ë°åÂàó„ÇíÂæó„Çã\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 4. ÂèØË¶ñÂåñÁî®ÔºöDataFrameÂåñ\n",
    "df_sim = pd.DataFrame(\n",
    "    similarity_matrix,\n",
    "    index=[f\"Post{i}\" for i in range(len(posts_akemashite_omedetou))],\n",
    "    columns=[f\"Post{i}\" for i in range(len(posts_akemashite_omedetou))]\n",
    ")\n",
    "\n",
    "print(\"=== „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶Ë°åÂàó ===\")\n",
    "display(df_sim)\n",
    "\n",
    "# 5. Áâπ„Å´‰ºº„Å¶„ÅÑ„ÇãÊäïÁ®ø„Éö„Ç¢„ÇíÊäΩÂá∫„Åô„Çã‰æã\n",
    "#   ÔºàËá™Â∑±È°û‰ººÂ∫¶=1.0„ÇíÈô§„Åç„ÄÅ‰∏ä‰Ωç„Å†„ÅëË¶ã„ÇãÔºâ\n",
    "similarities = []\n",
    "N = len(posts_akemashite_omedetou)\n",
    "for i in range(N):\n",
    "    for j in range(i+1, N):\n",
    "        sim = similarity_matrix[i, j]\n",
    "        similarities.append(((i, j), sim))\n",
    "\n",
    "# È°û‰ººÂ∫¶„ÇíÈôçÈ†Ü„ÇΩ„Éº„Éà\n",
    "similarities_sorted = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n=== ‰ºº„Å¶„ÅÑ„ÇãÊäïÁ®ø„Éö„Ç¢ (‰∏ä‰Ωç10) ===\")\n",
    "for (i, j), sim in similarities_sorted[:10]:\n",
    "    print(f\"Post{i} vs Post{j} : similarity={sim:.3f}\")\n",
    "    print(f\"   - {posts_akemashite_omedetou[i]}\")\n",
    "    print(f\"   - {posts_akemashite_omedetou[j]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803f1164-1736-47fd-871c-478fd676e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mecab-python3 in /home/y-matae/.local/lib/python3.12/site-packages (1.0.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f04216c-3e7d-4620-8a1d-fb8bfea10d68",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\n----------------------------------------------------------\n\nFailed initializing MeCab. Please see the README for possible solutions:\n\n    https://github.com/SamuraiT/mecab-python3#common-issues\n\nIf you are still having trouble, please file an issue here, and include the\nERROR DETAILS below:\n\n    https://github.com/SamuraiT/mecab-python3/issues\n\nissue„ÇíËã±Ë™û„ÅßÊõ∏„ÅèÂøÖË¶Å„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n\n------------------- ERROR DETAILS ------------------------\narguments: -Owakati\ndefault dictionary path: None\n[ifs] no such file or directory: /usr/local/etc/mecabrc\n----------------------------------------------------------\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/MeCab/__init__.py:137\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Tagger, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(args)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     33\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[1;32m     34\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenize_japanese,\n\u001b[1;32m     35\u001b[0m     token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# tokenizer„Çí‰Ωø„ÅÜÂ†¥Âêà„ÅØÊ≠£Ë¶èË°®Áèæ„Éë„Çø„Éº„É≥„ÇíÁÑ°ÂäπÂåñ\u001b[39;00m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 4) TF-IDFË°åÂàó„ÅÆ‰ΩúÊàê\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(posts_akemashite_omedetou)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 5) „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶„ÇíË®àÁÆó (N x NË°åÂàó)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mtokenize_japanese\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mMeCab„Çí‰Ωø„Å£„Å¶Êó•Êú¨Ë™ûÊñáÁ´†„ÇíÂçòË™û„Å´ÂàÜÂâ≤ÔºàÂàÜ„Åã„Å°Êõ∏„ÅçÔºâ„Åó„ÄÅ„Éà„Éº„ÇØ„É≥„ÅÆ„É™„Çπ„Éà„ÇíËøî„Åô„ÄÇ\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m‰æã:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m  \"Êòé„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô\" ‚Üí [\"Êòé„Åë\", \"„Åæ„Åó\", \"„Å¶\", \"„Åä„ÇÅ„Åß„Å®„ÅÜ\", \"„Åî„Åñ„ÅÑ\", \"„Åæ„Åô\"]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Tagger„ÅÆÂºïÊï∞„ÅØ‰ΩøÁî®„Åô„Çã„Ç™„Éó„Ç∑„Éß„É≥„ÇÑËæûÊõ∏„Å´„Çà„ÇäË™øÊï¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ‰æã: \"-Owakati\" „ÇÑ \"-Ochasen\"„Å™„Å©\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m mecab \u001b[38;5;241m=\u001b[39m MeCab\u001b[38;5;241m.\u001b[39mTagger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-Owakati\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m parsed \u001b[38;5;241m=\u001b[39m mecab\u001b[38;5;241m.\u001b[39mparse(text)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/MeCab/__init__.py:139\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Tagger, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(args)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_info(rawargs)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mee\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n----------------------------------------------------------\n\nFailed initializing MeCab. Please see the README for possible solutions:\n\n    https://github.com/SamuraiT/mecab-python3#common-issues\n\nIf you are still having trouble, please file an issue here, and include the\nERROR DETAILS below:\n\n    https://github.com/SamuraiT/mecab-python3/issues\n\nissue„ÇíËã±Ë™û„ÅßÊõ∏„ÅèÂøÖË¶Å„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n\n------------------- ERROR DETAILS ------------------------\narguments: -Owakati\ndefault dictionary path: None\n[ifs] no such file or directory: /usr/local/etc/mecabrc\n----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# #Âãï‰Ωú„Å´„ÅØMecab„ÅÆÂ∞éÂÖ•„ÅåÂøÖË¶Å„ÄÇ\n",
    "\n",
    "\n",
    "# import MeCab\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # 1) ÂΩ¢ÊÖãÁ¥†Ëß£ÊûêÁî®„ÅÆÈñ¢Êï∞„ÇíÂÆöÁæ©\n",
    "# def tokenize_japanese(text):\n",
    "#     \"\"\"\n",
    "#     MeCab„Çí‰Ωø„Å£„Å¶Êó•Êú¨Ë™ûÊñáÁ´†„ÇíÂçòË™û„Å´ÂàÜÂâ≤ÔºàÂàÜ„Åã„Å°Êõ∏„ÅçÔºâ„Åó„ÄÅ„Éà„Éº„ÇØ„É≥„ÅÆ„É™„Çπ„Éà„ÇíËøî„Åô„ÄÇ\n",
    "#     ‰æã:\n",
    "#       \"Êòé„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô\" ‚Üí [\"Êòé„Åë\", \"„Åæ„Åó\", \"„Å¶\", \"„Åä„ÇÅ„Åß„Å®„ÅÜ\", \"„Åî„Åñ„ÅÑ\", \"„Åæ„Åô\"]\n",
    "#     \"\"\"\n",
    "#     # Tagger„ÅÆÂºïÊï∞„ÅØ‰ΩøÁî®„Åô„Çã„Ç™„Éó„Ç∑„Éß„É≥„ÇÑËæûÊõ∏„Å´„Çà„ÇäË™øÊï¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "#     # ‰æã: \"-Owakati\" „ÇÑ \"-Ochasen\"„Å™„Å©\n",
    "#     mecab = MeCab.Tagger(\"-Owakati\")\n",
    "#     parsed = mecab.parse(text)\n",
    "#     if parsed is None:\n",
    "#         return []  # Ëß£Êûê„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØÁ©∫„É™„Çπ„Éà„ÇíËøî„Åô\n",
    "#     # ÊîπË°å„ÇíÈô§Âéª„Åó„Å¶„Çπ„Éö„Éº„ÇπÂå∫Âàá„Çä„Å´„Å™„Å£„Å¶„ÅÑ„Çã„Éà„Éº„ÇØ„É≥Âàó„Çí„É™„Çπ„ÉàÂåñ\n",
    "#     tokens = parsed.strip().split()\n",
    "#     return tokens\n",
    "\n",
    "# # 2) ÂàÜÊûêÂØæË±°„ÅÆÊñáÁ´†Ôºà„ÅÇ„Åë„Åä„ÇÅ„Éù„Çπ„Éà„ÅÆ‰æãÔºâ\n",
    "# posts_akemashite_omedetou = [\n",
    "#     \"Êòé„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅ‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ\",\n",
    "#     \"„ÅÇ„Åë„Åä„ÇÅ„Åß„ÅôÔºÅÁöÜ„Åï„Çì‰ªäÂπ¥„ÇÇ„Çà„Çç„Åó„Åè„Éº\",\n",
    "#     \"Happy new year to everyone! Wishing you all the best.\",\n",
    "#     \"„ÅÇ„Åë„Åæ„Åó„Å¶„Åä„ÇÅ„Åß„Å®„ÅÜÔºÅ‰ªäÂπ¥„ÅÆÊä±Ë≤†„ÅØ‰Ωï„Åß„Åô„ÅãÔºü\",\n",
    "#     \"„ÅÇ„Åë„Åä„ÇÅÔΩûÔºÅ‰ªäÂπ¥„ÇÇ„Åü„Åè„Åï„Çì„ÅäË©±„Åó„Åæ„Åó„Çá„ÅÜ„Å≠„ÄÇ\"\n",
    "# ]\n",
    "\n",
    "# # 3) TfidfVectorizer„Å´ MeCab„Éô„Éº„Çπ„ÅÆ tokenizer „ÇíÊ∏°„Åô\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     tokenizer=tokenize_japanese,\n",
    "#     token_pattern=None,  # tokenizer„Çí‰Ωø„ÅÜÂ†¥Âêà„ÅØÊ≠£Ë¶èË°®Áèæ„Éë„Çø„Éº„É≥„ÇíÁÑ°ÂäπÂåñ\n",
    "# )\n",
    "\n",
    "# # 4) TF-IDFË°åÂàó„ÅÆ‰ΩúÊàê\n",
    "# tfidf_matrix = vectorizer.fit_transform(posts_akemashite_omedetou)\n",
    "\n",
    "# # 5) „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶„ÇíË®àÁÆó (N x NË°åÂàó)\n",
    "# similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# # 6) ÁµêÊûú„ÇíÁ¢∫Ë™ç\n",
    "# print(\"=== TF-IDF ÂêÑÊñáÊõ∏„ÅÆ„Éô„ÇØ„Éà„É´Ê¨°ÂÖÉÊï∞ ===\")\n",
    "# print(tfidf_matrix.shape)  # ‰æã„Åà„Å∞ (5, X) „Å™„Å©\n",
    "# print()\n",
    "\n",
    "# print(\"=== „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶Ë°åÂàó ===\")\n",
    "# for i in range(similarity_matrix.shape[0]):\n",
    "#     row_sims = []\n",
    "#     for j in range(similarity_matrix.shape[1]):\n",
    "#         row_sims.append(f\"{similarity_matrix[i, j]:.3f}\")\n",
    "#     print(f\"Post{i}: \" + \"\\t\".join(row_sims))\n",
    "\n",
    "# print()\n",
    "# print(\"=== È°û‰ººÂ∫¶„ÅåÈ´ò„ÅÑÊäïÁ®ø„Éö„Ç¢„Çí‰∏ä‰ΩçË°®Á§∫ ===\")\n",
    "# # È°û‰ººÂ∫¶Ë°åÂàó„Åã„Çâ‰∏ä‰∏âËßí„Å†„Åë„ÇíÊäú„ÅçÂá∫„Åó„Å¶„ÇΩ„Éº„Éà\n",
    "# pairs = []\n",
    "# N = len(posts_akemashite_omedetou)\n",
    "# for i in range(N):\n",
    "#     for j in range(i+1, N):\n",
    "#         sim = similarity_matrix[i, j]\n",
    "#         pairs.append(((i, j), sim))\n",
    "\n",
    "# # „ÇΩ„Éº„ÉàÔºàÈ°û‰ººÂ∫¶„ÅÆÈôçÈ†ÜÔºâ\n",
    "# pairs_sorted = sorted(pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# for idx, (pair, sim) in enumerate(pairs_sorted[:10], start=1):\n",
    "#     i, j = pair\n",
    "#     print(f\"{idx}. Post{i} vs Post{j} = {sim:.3f}\")\n",
    "#     print(f\"   - {posts_akemashite_omedetou[i]}\")\n",
    "#     print(f\"   - {posts_akemashite_omedetou[j]}\")\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c32af1-729f-436f-9893-52075517ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶Ë°åÂàó ===\n",
      "Post0: 1.000\t0.139\t0.110\t0.359\t0.127\n",
      "Post1: 0.139\t1.000\t0.105\t0.174\t0.121\n",
      "Post2: 0.110\t0.105\t1.000\t0.138\t0.096\n",
      "Post3: 0.359\t0.174\t0.138\t1.000\t0.312\n",
      "Post4: 0.127\t0.121\t0.096\t0.312\t1.000\n",
      "\n",
      "=== È°û‰ººÂ∫¶„ÅåÈ´ò„ÅÑÊäïÁ®ø„Éö„Ç¢‰∏ä‰Ωç ===\n",
      "1. Post0 vs Post3 = 0.359\n",
      "   - Happy new year everyone! Wishing you all the best.\n",
      "   - Happy new year! Let's make this year great!\n",
      "\n",
      "2. Post3 vs Post4 = 0.312\n",
      "   - Happy new year! Let's make this year great!\n",
      "   - Let's celebrate the start of a brand new year together!\n",
      "\n",
      "3. Post1 vs Post3 = 0.174\n",
      "   - I hope you have a wonderful new year celebration.\n",
      "   - Happy new year! Let's make this year great!\n",
      "\n",
      "4. Post0 vs Post1 = 0.139\n",
      "   - Happy new year everyone! Wishing you all the best.\n",
      "   - I hope you have a wonderful new year celebration.\n",
      "\n",
      "5. Post2 vs Post3 = 0.138\n",
      "   - This is unrelated to new year greetings, just a test post.\n",
      "   - Happy new year! Let's make this year great!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ‰æã: Ëã±Ë™û„ÅÆ„Åø„ÅÆ‚ÄúHappy New Year‚ÄùÈñ¢ÈÄ£ÊäïÁ®øÔºà„Çµ„É≥„Éó„É´Ôºâ\n",
    "posts_english = [\n",
    "    \"Happy new year everyone! Wishing you all the best.\",\n",
    "    \"I hope you have a wonderful new year celebration.\",\n",
    "    \"This is unrelated to new year greetings, just a test post.\",\n",
    "    \"Happy new year! Let's make this year great!\",\n",
    "    \"Let's celebrate the start of a brand new year together!\"\n",
    "]\n",
    "\n",
    "# 1. TF-IDF„Éô„ÇØ„Éà„É´Âåñ\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',  # Ëã±Ë™û„ÅÆ„Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„Éâ„ÇíÈô§Â§ñ„Åô„ÇãÂ†¥Âêà\n",
    "    lowercase=True         # Ëã±Ë™û„ÇíÂ∞èÊñáÂ≠óÂåñÔºàÊó¢ÂÆö„ÅßTrueÔºâ\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(posts_english)\n",
    "# shape=(‰ª∂Êï∞, ÂçòË™ûÊ¨°ÂÖÉ)\n",
    "\n",
    "# 2. „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶„ÅÆË®àÁÆó\n",
    "sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "# shape=(‰ª∂Êï∞, ‰ª∂Êï∞)\n",
    "\n",
    "# 3. Âá∫ÂäõÁ¢∫Ë™ç\n",
    "print(\"=== „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶Ë°åÂàó ===\")\n",
    "num_posts = len(posts_english)\n",
    "for i in range(num_posts):\n",
    "    row_sims = []\n",
    "    for j in range(num_posts):\n",
    "        row_sims.append(f\"{sim_matrix[i, j]:.3f}\")\n",
    "    print(f\"Post{i}: \" + \"\\t\".join(row_sims))\n",
    "\n",
    "# 4. ‰∏ä‰Ωç„Å´‰ºº„Å¶„ÅÑ„ÇãÊäïÁ®ø„Éö„Ç¢„ÇíË°®Á§∫\n",
    "pairs = []\n",
    "for i in range(num_posts):\n",
    "    for j in range(i + 1, num_posts):\n",
    "        pairs.append(((i, j), sim_matrix[i, j]))\n",
    "\n",
    "# È°û‰ººÂ∫¶ÈôçÈ†Ü„Å´„ÇΩ„Éº„Éà\n",
    "pairs_sorted = sorted(pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n=== È°û‰ººÂ∫¶„ÅåÈ´ò„ÅÑÊäïÁ®ø„Éö„Ç¢‰∏ä‰Ωç ===\")\n",
    "for idx, (pair, sim) in enumerate(pairs_sorted[:5], start=1):\n",
    "    i, j = pair\n",
    "    print(f\"{idx}. Post{i} vs Post{j} = {sim:.3f}\")\n",
    "    print(f\"   - {posts_english[i]}\")\n",
    "    print(f\"   - {posts_english[j]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53120bab-df99-421a-953e-427fffe4e6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a70b54-8674-4703-bbe0-b570a482deab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce78075-1f42-4221-b174-efb528f1cfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97ed0a-c956-4eb4-b04c-9a2c3e12ae04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
